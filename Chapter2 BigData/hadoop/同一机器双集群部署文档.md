[TOC]

# 【HDP】同一机器双集群部署文档

## 1.准备两个不同的用户

由于需要区分环境变量，所以准备两个不同的用户用于服务启动以及环境变量配置

```shell
1.添加用户
adduser deploy2
passwd deploy2

2.赋予root权限
root用户在/etc/sudoers下添加一行
deploy2  ALL=(ALL)      NOPASSWD: ALL
```



## 2.部署两套hdp客户端

### 部署客户端基本操作

```shell
说明：
1. 以下操作请在执行代理服务器上用root权限执行。
2. $hdp_ip为集群任意节点的ip。
3. 具体操作可能因版本不同而有所变化，请根据实际情况操作。

sudo mkdir -p /usr/hdp/
sudo rsync -av root@$hdp_ip:/usr/hdp/* /usr/hdp/
sudo rsync -av root@$hdp_ip:/etc/hadoop /etc/
sudo rsync -av root@$hdp_ip:/etc/hive /etc/
sudo rsync -av root@$hdp_ip:/etc/spark* /etc/
sudo rsync -av root@$hdp_ip:/etc/hbase* /etc/
```

### 两个客户端兼容操作

部署两个客户端，会有一些文件夹有冲突

- /etc/hadoop目录下conf目录
- /etc/hive目录下conf目录
- /etc/spark*目录下conf目录
- /etc/hbase目录下conf目录
- /usr/hdp目录下current目录和share目录

以上目录需要改名，防止覆盖

```
以hdp2.6.0.3-8和hdp3.1.0.0-78为例，部署完的集群客户端目录结构示例如下

/usr/hdp/
├── 2.6.0.3-8
├── 3.1.0.0-78
├── current2
├── current3
├── share2
└── share3

/etc/hadoop/
├── 2.6.0.3-8
├── 3.1.0.0-78
├── conf2 -> /usr/hdp/current2/hadoop-client/conf
└── conf3 -> /usr/hdp/current3/hadoop-client/conf

/etc/hbase/
├── 2.6.0.3-8
├── 3.1.0.0-78
├── conf2 -> /usr/hdp/current2/hbase-client/conf
└── conf3 -> /usr/hdp/current3/hbase-client/conf

/etc/hive
├── 2.6.0.3-8
├── 3.1.0.0-78
├── conf2 -> /usr/hdp/current2/hive-client/conf
└── conf3 -> /usr/hdp/current3/hive-client/conf

/etc/spark
├── 2.6.0.3-8
└── conf -> /usr/hdp/current2/spark-client/conf

/etc/spark2
├── 2.6.0.3-8
├── 3.1.0.0-78
├── conf2 -> /usr/hdp/current2/spark2-client/conf
└── conf3 -> /usr/hdp/current3/spark2-client/conf

```

由于目录修改，配置文件中写死的一些路径需要修改

```shell
sed -i s/"\/etc\/hadoop\/conf"/"\/etc\/hadoop\/conf2"/g `grep '/etc/hadoop/conf' -Rl /usr/hdp/2.6.0.3-8`
sed -i s/"\/etc\/hadoop\/conf22"/"\/etc\/hadoop\/conf2"/g `grep '/etc/hadoop/conf22' -Rl /usr/hdp/2.6.0.3-8`
sed -i s/"\/usr\/hdp\/current"/"\/usr\/hdp\/current2"/g `grep '/usr/hdp/current' -Rl /usr/hdp/2.6.0.3-8`
sed -i s/"\/usr\/hdp\/current22"/"\/usr\/hdp\/current2"/g `grep '/usr/hdp/current' -Rl /usr/hdp/2.6.0.3-8`
sed -i s/"\/etc\/hadoop\/conf"/"\/etc\/hadoop\/conf2"/g `grep '/etc/hadoop/conf' -Rl /usr/hdp/current2`
sed -i s/"\/etc\/hadoop\/conf22"/"\/etc\/hadoop\/conf2"/g `grep '/etc/hadoop/conf22' -Rl /usr/hdp/current2`

sed -i s/"\/etc\/hadoop\/conf"/"\/etc\/hadoop\/conf3"/g `grep '/etc/hadoop/conf' -Rl /usr/hdp/current3`
sed -i s/"\/etc\/hadoop\/conf33"/"\/etc\/hadoop\/conf3"/g `grep '/etc/hadoop/conf33' -Rl /usr/hdp/current3`
sed -i s/"\/etc\/hadoop\/conf"/"\/etc\/hadoop\/conf3"/g `grep '/etc/hadoop/conf' -Rl /usr/hdp/3.1.0.0-78`
sed -i s/"\/etc\/hadoop\/conf33"/"\/etc\/hadoop\/conf3"/g `grep '/etc/hadoop/conf33' -Rl /usr/hdp/3.1.0.0-78`
sed -i s/"\/usr\/hdp\/current"/"\/usr\/hdp\/current3"/g `grep '/usr/hdp/current' -Rl /usr/hdp/3.1.0.0-78`
sed -i s/"\/usr\/hdp\/current33"/"\/usr\/hdp\/current3"/g `grep '/usr/hdp/current' -Rl /usr/hdp/3.1.0.0-78`
sed -i s/"\/etc\/hadoop\/conf"/"\/etc\/hadoop\/conf3"/g `grep '/etc/hadoop/conf' -Rl /usr/hdp/current3`
sed -i s/"\/etc\/hadoop\/conf33"/"\/etc\/hadoop\/conf3"/g `grep '/etc/hadoop/conf22' -Rl /usr/hdp/current3`

```

需要准备两个路径存放keytab文件

例如/etc/securuty/keytabs 和 /etc/security/keytabs_HDP3/

两个集群的krb5文件需要合并,以下是一个合并后的krb5文件示例

```
[libdefaults]
  renew_lifetime = 7d
  forwardable = true
  default_realm = DTWAVE.COM
  ticket_lifetime = 24h
  dns_lookup_realm = false
  dns_lookup_kdc = false
  default_ccache_name = /tmp/krb5cc_%{uid}
  #default_tgs_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5
  #default_tkt_enctypes = aes des3-cbc-sha1 rc4 des-cbc-md5

[realms]
  DTWAVE = {
    admin_server = hdp-2.6-node2.dtwave.com
    kdc = hdp-2.6-node2.dtwave.com
  }
  DTWAVE.COM = {
    admin_server = hdp.master.dtwave
    kdc = hdp.master.dtwave
  }

[domain_realm]
 dtwave.com=DTWAVE
 .dtwave.com=DTWAVE
 master.dtwave=DTWAVE.COM
 .master.dtwave=DTWAVE.COM
```

## 3.用户环境变量配置

```shell
#HDP_VERSION因版本不同而有所变化，请根据实际情况配置。
#先切换到指定用户，然后把hdp的环境变量写入到该用户的环境变量中
#注意修改下面脚本中deploy为当前用户名

echo '
export HDP_VERSION=2.6.0.3-8
export HADOOP_COMMON_HOME=/usr/hdp/$HDP_VERSION/hadoop
export HADOOP_HDFS_HOME=/usr/hdp/$HDP_VERSION/hadoop-hdfs
export HADOOP_MAPRED_HOME=/usr/hdp/$HDP_VERSION/hadoop-mapreduce
export HADOOP_YARN_HOME=/usr/hdp/$HDP_VERSION/hadoop-yarn
export HIVE_HOME=/usr/hdp/$HDP_VERSION/hive
export HBASE_HOME=/usr/hdp/$HDP_VERSION/hbase
export SPARK_HOME=/usr/hdp/$HDP_VERSION/spark2
export SQOOP_HOME=/usr/hdp/$HDP_VERSION/sqoop
PATH=$PATH:$SPARK_HOME/bin:$HADOOP_COMMON_HOME/bin:$HADOOP_HDFS_HOME/bin:$HADOOP_MAPRED_HOME/bin:$HIVE_HOME/bin:$HBASE_HOME/bin:/usr/local/bin/python/bin:$STORM_HOME/bin:$SQOOP_HOME/bin' | sudo tee -a /home/deploy/.bashrc > /dev/null

echo 'spark.ui.enabled false' >> /etc/spark2/conf/spark-defaults.conf

echo 'log4j.logger.org.apache.hadoop.yarn.client=INFO' >> /etc/spark2/conf/log4j.properties

source /home/deploy/.bashrc
```



## 4.服务部署修改点

本次部署有三个服务需要部署两份

- dsource
- node
- datax

其中dsource和node是需要**使用两个用户，启动两个进程**

涉及兼容的改动点如下

dsource配置文件改动点

```
logging.path=/data/dsource-HDP3/logs
security.krb5.conf.path=/etc/HDP3/krb5.conf
kerberos.keytab.dir:/etc/security/keytabs_HDP3/
server.port=8785
以及所有涉及keytab的路径
```

dubhe-node配置文件改动点

```
logging.path=/data/dubhe-node-HDP3/logs
node.datax=/opt/third/datax
keytab.dir=/etc/security/keytabs_HDP3/
node.exec.baseDir=/data/dubhe-node-HDP3/codes/
node.port=8713
server.port=8713
以及所有涉及keytab的路径
```

datax 

配置文件位置

需要把集群配置文件：hive-site.xml,hbase-site.xml,

.secret.properties文件改动点

```
security.krb5.conf.path=/etc/HDP3/krb5.conf
kerberos.keytab.dir=/etc/security/keytabs_HDP3/
cluster.conf.dir=/opt/third/conf/
hbase.keyTab=/etc/security/keytabs_HDP3/shulan_admin.keytab
以及所有涉及keytab的路径
```

# [CDH6.3.1]部署

环境准备